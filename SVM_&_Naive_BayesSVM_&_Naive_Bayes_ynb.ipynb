{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxbNKELFF_kc"
      },
      "outputs": [],
      "source": [
        "1.What is a Support Vector Machine (SVM), and how does it work?\n",
        "Ans..A Support Vector Machine (SVM) is a supervised machine learning algorithm that is widely used for classification and, to some extent, regression tasks. Its main idea is to find the best decision boundary (called a hyperplane) that separates data points belonging to different classes.\n",
        "How SVM Works\n",
        "\n",
        "Decision Boundary (Hyperplane):\n",
        "\n",
        "In 2D, the hyperplane is a line.\n",
        "\n",
        "In 3D, it’s a plane.\n",
        "\n",
        "In higher dimensions, it’s a hyperplane.\n",
        "SVM tries to find the hyperplane that best separates the classes.\n",
        "Maximizing the Margin:\n",
        "\n",
        "Instead of just separating the classes, SVM chooses the boundary that has the maximum margin — i.e., the greatest distance from the nearest points of both classes.\n",
        "\n",
        "The nearest data points to the hyperplane are called support vectors. These are the most critical points in determining the boundary.\n",
        "Linearly Separable Data:\n",
        "\n",
        "If the data can be separated by a straight line (or hyperplane), SVM finds that maximum-margin separator.\n",
        "Non-linearly Separable Data:\n",
        "\n",
        "Real-world data often isn’t linearly separable.\n",
        "\n",
        "SVM handles this using the kernel trick: it maps data into a higher-dimensional space where a linear separator can exist.\n",
        "Common kernels:\n",
        "\n",
        "Linear kernel (straight line separation).\n",
        "\n",
        "Polynomial kernel (curved boundaries).\n",
        "\n",
        "Radial Basis Function (RBF) / Gaussian kernel (very flexible, handles complex boundaries).\n",
        "Soft Margin (C parameter):\n",
        "\n",
        "In practice, some misclassifications are allowed to improve generalization.\n",
        "\n",
        "The C parameter controls the trade-off:\n",
        "\n",
        "High C → tries to classify all points correctly (risk of overfitting).\n",
        "\n",
        "Low C → allows more margin violations (better generalization).\n",
        "SVM Intuition with Example\n",
        "\n",
        "Imagine you have two classes of points (red and blue) in 2D space.\n",
        "\n",
        "A simple classifier might draw any line that separates them.\n",
        "\n",
        "SVM finds the line that maximizes the gap (margin) between the two groups, ensuring robustness against small variations in data.\n",
        "Strengths of SVM\n",
        "\n",
        "Effective in high-dimensional spaces (e.g., text classification, bioinformatics).\n",
        "\n",
        "Works well even when number of features > number of samples.\n",
        "\n",
        "Very flexible with different kernel functions.\n",
        "Weaknesses of SVM\n",
        "\n",
        "Training can be slow on very large datasets.\n",
        "\n",
        "Choice of kernel and hyperparameters (C, gamma) is crucial.\n",
        "\n",
        "Doesn’t perform well when there’s a lot of overlap/noise between classes.\n",
        "SVM finds the optimal separating hyperplane with the maximum margin between classes. With the kernel trick, it can handle even complex, non-linear classification problems effectively."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2. Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "ANS..1. Hard Margin SVM\n",
        "\n",
        "Definition: A strict version of SVM where the hyperplane must separate the classes perfectly with no misclassifications.\n",
        "\n",
        "Works only when data is linearly separable.\n",
        "\n",
        "It maximizes the margin while ensuring that all training points are correctly classified.\n",
        "\n",
        "Mathematical constraint:\n",
        "For each training point (x\n",
        "i\n",
        "\t​\n",
        "\n",
        ",y\n",
        "i\n",
        "\t​\n",
        "\n",
        "):yi​⋅(w⋅xi​+b)≥1\n",
        "(Here,\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "\t​\n",
        "\n",
        " is the class label: +1 or -1.)\n",
        "Advantages:\n",
        "\n",
        "Gives a clear decision boundary.\n",
        "\n",
        "Useful when data is clean and noise-free\n",
        "Disadvantages:\n",
        "\n",
        "Very sensitive to outliers (even a single mislabeled point can break the separation).\n",
        "\n",
        "Not suitable for real-world noisy datasets.\n",
        "\n",
        "Soft Margin SVM\n",
        "\n",
        "Definition: A relaxed version of SVM that allows some misclassifications (margin violations) to achieve better generalization.\n",
        "\n",
        "Introduces slack variables (\n",
        "𝜉\n",
        "𝑖\n",
        "ξ\n",
        "i\n",
        "\t​\n",
        "\n",
        ") to allow points inside the margin or even misclassified.\n",
        "\n",
        "Controlled by the C parameter\n",
        "High C → less tolerance for errors (behaves closer to hard margin).\n",
        "\n",
        "Low C → more tolerance for errors (wider margin, better generalization).\n",
        "Mathematical constraint:yi​⋅(w⋅xi​+b)≥1−ξi​,ξi​≥0\n",
        "(Here,\n",
        "𝜉\n",
        "𝑖\n",
        "ξ\n",
        "i\n",
        "\t​\n",
        "\n",
        " measures the violation of the margin for each point.)\n",
        "Advantages:\n",
        "\n",
        "Works well with noisy, overlapping, and real-world data.\n",
        "\n",
        "More robust to outliers.\n",
        "Disadvantages:\n",
        "\n",
        "Needs proper tuning of C.\n",
        "\n",
        "Might allow too many errors if C is too small.\n",
        "Visual Intuition\n",
        "\n",
        "Hard Margin: A strict line that separates all points correctly (works only if data is perfectly separable).\n",
        "\n",
        "Soft Margin: A line with some tolerance, allowing a few points inside the margin or even misclassified, to balance accuracy and generalization.\n",
        "In short:\n",
        "\n",
        "Hard Margin = Zero tolerance for mistakes (ideal for clean, separable data).\n",
        "\n",
        "Soft Margin = Allows controlled mistakes (better for real-world noisy data)."
      ],
      "metadata": {
        "id": "w1MYMCL9HOIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3.: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "ANS..Many real-world datasets are not linearly separable (you can’t draw a straight line/hyperplane to separate the classes).\n",
        "\n",
        "The kernel trick allows SVM to handle such data by implicitly mapping the original input features into a higher-dimensional feature space, where the data becomes linearly separable.\n",
        "\n",
        "Importantly:\n",
        "\n",
        "We don’t compute this transformation explicitly (which might be computationally expensive).\n",
        "\n",
        "Instead, we use a kernel function that computes the dot product in the higher-dimensional space directly from the original space.\n",
        "This avoids the curse of dimensionality and makes non-linear classification efficient.\n",
        "Example of a Kernel: Radial Basis Function (RBF) / Gaussian Kernel\n",
        "\n",
        "Formula:K(x,x′)=exp(−γ∥x−x′∥2)\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        "x,x\n",
        "′are two data points,\n",
        "γ controls the spread of the kernel (how far the influence of a point extends).\n",
        "Intuition:\n",
        "\n",
        "Measures the similarity between two points.\n",
        "\n",
        "If points are very close, the kernel value is near 1.\n",
        "\n",
        "If far apart, the kernel value is near 0.\n",
        "Use Case:\n",
        "The RBF kernel is very popular for handling complex, non-linear boundaries.\n",
        "Example:\n",
        "\n",
        "Classifying data shaped like concentric circles or spirals.\n",
        "\n",
        "A linear hyperplane in the original space can’t separate these, but in a higher-dimensional space (using RBF kernel), SVM can draw a clean separating boundary.\n",
        "Quick Analogy\n",
        "\n",
        "Imagine trying to separate red and blue dots arranged in two concentric circles:\n",
        "\n",
        "In 2D, no straight line works.\n",
        "\n",
        "The kernel trick lifts the data into 3D (adding a new dimension like radius).\n",
        "\n",
        "In 3D, now a plane can separate the two circles easily.\n",
        "In summary:\n",
        "The kernel trick lets SVM handle non-linear problems by mapping data to higher dimensions without explicitly computing the transformation.\n",
        "\n",
        "Example: RBF kernel is widely used for datasets where classes are not linearly separable, like circular or irregular boundaries."
      ],
      "metadata": {
        "id": "kpqd4cEfIfyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4.What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "ANS..The Naïve Bayes Classifier is a probabilistic machine learning model based on Bayes’ Theorem. It is commonly used for classification tasks, especially in text classification (like spam filtering, sentiment analysis, topic categorization).\n",
        "Bayes’ Theorem states:P(Y∣X)=P(X)P(X∣Y)⋅P(Y)​\n",
        "Where:\n",
        "\n",
        "𝑌\n",
        "Y = class (e.g., Spam or Not Spam)\n",
        "\n",
        "𝑋\n",
        "X = features (e.g., words in an email)\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "P(Y∣X) = probability of class given the features (posterior probability)\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "P(X∣Y) = probability of features given the class (likelihood)\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        ")\n",
        "P(Y) = prior probability of the class\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(X) = probability of features\n",
        "The classifier predicts the class\n",
        "𝑌\n",
        "Y with the highest posterior probability.\n",
        "is it called “Naïve”?\n",
        "\n",
        "Because it makes a strong simplifying assumption:\n",
        "\n",
        "It assumes that all features are conditionally independent given the class label.\n",
        "\n",
        "For example:\n",
        "\n",
        "In spam detection, if an email has the words “offer” and “free,” Naïve Bayes assumes that the probability of these two words appearing together in spam is simply the product of their individual probabilities.\n",
        "\n",
        "In reality, words in language are often correlated (not independent), but Naïve Bayes still works surprisingly well despite this “naïve” assumption.\n",
        "Types of Naïve Bayes Models\n",
        "\n",
        "Multinomial Naïve Bayes – used for text classification (word counts/frequencies).\n",
        "\n",
        "Bernoulli Naïve Bayes – used when features are binary (word present or not).\n",
        "\n",
        "Gaussian Naïve Bayes – used when features are continuous and assumed to follow a normal distribution.\n",
        "Advantages\n",
        "\n",
        "Very fast to train and predict.\n",
        "\n",
        "Works well with high-dimensional data (e.g., text with thousands of features).\n",
        "\n",
        "Requires less training data compared to other algorithms.\n",
        "Disadvantages\n",
        "\n",
        "The “naïve” independence assumption is rarely true in real-world data.\n",
        "\n",
        "If a feature never appears with a class in the training data, the probability becomes zero (usually fixed with Laplace smoothing).\n",
        "In summary:\n",
        "The Naïve Bayes classifier applies Bayes’ theorem with the naïve assumption that features are independent. Despite being an oversimplification, it performs surprisingly well in practice, especially for text classification.\n"
      ],
      "metadata": {
        "id": "L9TN6-RZIfhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5.: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "ANS..Gaussian Naïve Bayes\n",
        "\n",
        "Assumption: Features are continuous and follow a normal (Gaussian) distribution.\n",
        "\n",
        "Formula for likelihood:P(xi​∣y)=2πσy2​\n",
        "​1​exp(−2σy2​(xi​−μy​)2​)\n",
        "where\n",
        "𝜇\n",
        "𝑦\n",
        "μ\n",
        "y\n",
        "\t​\n",
        "\n",
        " and\n",
        "𝜎\n",
        "𝑦\n",
        "2\n",
        "σ\n",
        "y\n",
        "2\n",
        "\t​\n",
        "\n",
        " are the mean and variance of feature\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "\t​\n",
        "\n",
        " for class\n",
        "𝑦\n",
        "y.When to use:\n",
        "\n",
        "Data with continuous features.\n",
        "\n",
        "Example:\n",
        "\n",
        "Medical diagnosis (height, weight, blood pressure).\n",
        "\n",
        "Iris dataset (petal length, sepal width).\n",
        "Multinomial Naïve Bayes\n",
        "\n",
        "Assumption: Features are discrete counts (non-negative integers).\n",
        "\n",
        "Typically used for word counts in text classification.\n",
        "\n",
        "Formula for likelihood:\n",
        "P(xi​∣y)=∏j​(xj​!)(∑j​xj​)!​j∏​θy,jxj​​\n",
        "x\n",
        "j\n",
        "\t​\n",
        "\n",
        " = count of word\n",
        "𝑗\n",
        "j, and\n",
        "𝜃\n",
        "𝑦\n",
        ",\n",
        "𝑗\n",
        "θ\n",
        "y,j\n",
        "\t​\n",
        "\n",
        " = probability of word\n",
        "𝑗\n",
        "j given class\n",
        "𝑦\n",
        "y.\n",
        "             When to use:\n",
        "\n",
        "Text classification with count features.\n",
        "\n",
        "Example:\n",
        "\n",
        "Spam vs. not spam (emails).\n",
        "\n",
        "Sentiment analysis (positive/negative reviews).\n",
        "\n",
        "Topic categorization (news classification).\n",
        "             Bernoulli Naïve Bayes\n",
        "\n",
        "Assumption: Features are binary (0/1 → presence or absence).\n",
        "\n",
        "Looks at whether a feature occurs or not, not how many times.\n",
        "\n",
        "Formula for likelihood:P(xi​∣y)=θyxi​​(1−θy​)(1−xi​)\n",
        "where\n",
        "𝜃\n",
        "𝑦\n",
        "θ\n",
        "y\n",
        "\t​\n",
        "\n",
        " = probability that feature = 1 given class\n",
        "𝑦\n",
        "y.\n",
        "\n",
        " When to use:Binary feature problems.\n",
        "\n",
        "Example:\n",
        "\n",
        "Email classification: Does the email contain the word “free”? (yes/no).\n",
        "\n",
        "Document classification when you only care about presence/absence of words, not frequency.\n",
        "             | Variant         | Feature Type             | Common Use Case                                                 |\n",
        "| --------------- | ------------------------ | --------------------------------------------------------------- |\n",
        "| **Gaussian**    | Continuous (real-valued) | Medical data, sensor data, Iris dataset                         |\n",
        "| **Multinomial** | Discrete counts          | Text classification (word counts, bag-of-words)                 |\n",
        "| **Bernoulli**   | Binary (0/1)             | Text classification (word presence/absence), document filtering |\n",
        "Rule of thumb:\n",
        "\n",
        "Use Gaussian NB for numeric/continuous data.\n",
        "\n",
        "Use Multinomial NB for text data with counts.\n",
        "\n",
        "Use Bernoulli NB for text data with binary features (presence/absence)."
      ],
      "metadata": {
        "id": "DV4gY7rPKVDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6.Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors\n",
        "\n",
        "ANS..from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train an SVM Classifier with linear kernel\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict on test data\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# 5. Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# 6. Print support vectors\n",
        "print(\"Support Vectors:\\n\", svm_model.support_vectors_)\n",
        "this program does:\n",
        "\n",
        "Loads the Iris dataset (a classic dataset with 150 samples of flower measurements).\n",
        "\n",
        "Splits data into 70% training, 30% testing.\n",
        "\n",
        "Trains an SVM with a linear kernel.\n",
        "\n",
        "Prints the accuracy score on test data.\n",
        "\n",
        "Displays the support vectors chosen by the model."
      ],
      "metadata": {
        "id": "PR10DRwCDbBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7.Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "ANS..from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict on test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 5. Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "What this program does:\n",
        "\n",
        "Loads the Breast Cancer Wisconsin dataset (569 samples, 30 features).\n",
        "\n",
        "Splits it into training (70%) and testing (30%) sets.\n",
        "\n",
        "Trains a Gaussian Naïve Bayes classifier.\n",
        "\n",
        "Generates a classification report showing precision, recall, F1-score, and support for both classes (malignant and benign)."
      ],
      "metadata": {
        "id": "iJqUwnoXD7CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8.: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n",
        "ANS..from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# 2. Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],       # Regularization parameter\n",
        "    'gamma': [0.001, 0.01, 0.1, 1], # Kernel coefficient\n",
        "    'kernel': ['rbf']             # Use RBF kernel\n",
        "}\n",
        "\n",
        "# 4. Train SVM with GridSearchCV\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict on test data\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# 6. Print best parameters and accuracy\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "What this program does:\n",
        "\n",
        "Loads the Wine dataset (178 samples, 13 features, 3 classes).\n",
        "\n",
        "Splits into 70% training / 30% testing.\n",
        "\n",
        "Uses GridSearchCV to tune:\n",
        "\n",
        "C (regularization strength).\n",
        "\n",
        "gamma (RBF kernel spread).\n",
        "\n",
        "Trains the best SVM model.\n",
        "\n",
        "Prints the best hyperparameters and the test accuracy.\n"
      ],
      "metadata": {
        "id": "U4k3gKDAEcIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9.: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "ANS..from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 1. Load a subset of the 20 Newsgroups dataset (binary classification for ROC-AUC)\n",
        "categories = ['sci.space', 'rec.sport.baseball']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target  # Binary labels (0 or 1)\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# 4. Train a Multinomial Naïve Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# 5. Predict probabilities for ROC-AUC\n",
        "y_prob = nb_model.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "# 6. Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n",
        "What this program does:\n",
        "\n",
        "Loads two categories (sci.space vs rec.sport.baseball) from the 20 Newsgroups dataset → makes it a binary classification problem.\n",
        "\n",
        "Splits into training (70%) and testing (30%).\n",
        "\n",
        "Uses TF-IDF to convert raw text into numeric features.\n",
        "\n",
        "Trains a Multinomial Naïve Bayes classifier (best for text counts/frequencies).\n",
        "\n",
        "Predicts probabilities and computes the ROC-AUC score."
      ],
      "metadata": {
        "id": "zKit8HUQFSyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10.: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "\n",
        "ANS..1. Preprocessing the Data\n",
        "\n",
        "Emails are unstructured text, so preprocessing is crucial.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Text Cleaning\n",
        "\n",
        "Remove HTML tags, special characters, numbers, and stopwords.\n",
        "\n",
        "Normalize case (lowercasing).\n",
        "\n",
        "Apply lemmatization/stemming to reduce words to their root forms.\n",
        "\n",
        "Handling Missing Data\n",
        "\n",
        "If some emails have missing text (e.g., only metadata):\n",
        "\n",
        "Option 1: Discard if very few.\n",
        "\n",
        "Option 2: Replace missing with placeholder text like \"missing\" so the model knows it exists.\n",
        "\n",
        "Text Vectorization\n",
        "\n",
        "Use TF-IDF vectorization → accounts for word frequency while reducing the weight of common words (“the”, “and”).\n",
        "\n",
        "Could also try word embeddings (Word2Vec, GloVe, BERT) for richer semantic meaning if computationally feasible\n",
        "2. Choosing a Model\n",
        "\n",
        "We’re deciding between SVM and Naïve Bayes.\n",
        "\n",
        "Naïve Bayes (Multinomial/Bernoulli):\n",
        "\n",
        "Simple, fast, effective for text classification.\n",
        "\n",
        "Works well with high-dimensional sparse data (like TF-IDF).\n",
        "\n",
        "Strong baseline for spam detection.\n",
        "\n",
        "SVM (with linear kernel):\n",
        "\n",
        "Works well in high-dimensional feature space.\n",
        "\n",
        "Often achieves higher accuracy on text classification tasks than NB.\n",
        "\n",
        "But more computationally expensive on large datasets.\n",
        "\n",
        " Approach:\n",
        "\n",
        "Start with Multinomial Naïve Bayes as a baseline (fast, interpretable).\n",
        "\n",
        "Then test Linear SVM for potentially higher accuracy if latency isn’t critical.\n",
        "\n",
        " 3. Handling Class Imbalance\n",
        " Spam datasets are often imbalanced (few spam, many legitimate emails).\n",
        "\n",
        "Techniques:\n",
        "\n",
        "Resampling:\n",
        "\n",
        "Oversample minority class (SMOTE).\n",
        "\n",
        "Undersample majority class.\n",
        "\n",
        "Class Weights:\n",
        "\n",
        "In SVM, set class_weight=\"balanced\".\n",
        "\n",
        "Threshold Adjustment:\n",
        "\n",
        "Instead of default 0.5 probability cutoff, tune decision threshold to improve recall for spam.\n",
        "\n",
        " Since misclassifying spam as legitimate is worse than the reverse, we’d bias towards higher recall for spam.\n",
        " 4. Model Evaluation\n",
        "\n",
        "Accuracy isn’t enough for imbalanced problems.\n",
        "\n",
        "Metrics to use:\n",
        "\n",
        "Precision (Spam class): Of emails predicted as spam, how many are truly spam?\n",
        "\n",
        "Recall (Spam class): Of actual spam emails, how many did we catch?\n",
        "\n",
        "F1-score: Balance between precision and recall.\n",
        "\n",
        "ROC-AUC / PR-AUC: Overall ability to separate classes.\n",
        "\n",
        "Confusion Matrix: To understand false positives (ham → spam) vs. false negatives (spam → ham).\n",
        "5. Business Impact\n",
        "\n",
        "A well-tuned spam classifier directly impacts business operations:\n",
        "\n",
        " Customer trust: Prevents legitimate users from missing important messages by minimizing false positives.\n",
        "\n",
        " Efficiency: Reduces manual filtering of spam, saving time.\n",
        "\n",
        " Security: Filters phishing/malicious emails, protecting users and the company’s reputation.\n",
        "\n",
        " Scalability: Automated pipeline can handle millions of emails daily with low latency.\n",
        " Final Plan\n",
        "\n",
        "Preprocess emails (clean, vectorize with TF-IDF).\n",
        "\n",
        "Train Naïve Bayes as baseline; then test Linear SVM.\n",
        "\n",
        "Handle imbalance with class weighting and threshold tuning.\n",
        "\n",
        "Evaluate with precision, recall, F1, ROC-AUC (not just accuracy).\n",
        "\n",
        "Deploy the model to filter emails in real time, retraining periodically as spam tactics evolve."
      ],
      "metadata": {
        "id": "ORm74mnhHU-R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}